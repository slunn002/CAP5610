{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to pytorch\n",
    "\n",
    "In this example, we'll take a look at how to use the pytorch framework to create a simple 3 layer ANN to classify the MNIST data set.\n",
    "\n",
    "## pytorch MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = './images'\n",
    "\n",
    "transformer = transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5,),(1.0,))])\n",
    "\n",
    "train_set = datasets.MNIST(root=image_folder, train=True, transform=transformer, download=True)\n",
    "test_set = datasets.MNIST(root=image_folder, train=False, transform=transformer, download=True)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 500)\n",
    "        self.fc2 = nn.Linear(500, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x1 = F.relu(self.fc1(x))\n",
    "        x2 = F.relu(self.fc2(x1))\n",
    "        x3 = self.fc3(x2)\n",
    "#         set_trace()\n",
    "        return F.log_softmax(x3,1)\n",
    "    \n",
    "    def name(self):\n",
    "        return \"MLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 100, train loss: 2.241422, acc: 0.228\n",
      "==>>> epoch: 0, batch index: 200, train loss: 2.139243, acc: 0.340\n",
      "==>>> epoch: 0, batch index: 300, train loss: 1.938830, acc: 0.422\n",
      "==>>> epoch: 0, batch index: 400, train loss: 1.655323, acc: 0.474\n",
      "==>>> epoch: 0, batch index: 500, train loss: 1.300390, acc: 0.519\n",
      "==>>> epoch: 0, batch index: 600, train loss: 1.014521, acc: 0.559\n",
      "==>>> epoch: 0, batch index: 700, train loss: 0.840627, acc: 0.592\n",
      "==>>> epoch: 0, batch index: 800, train loss: 0.727700, acc: 0.619\n",
      "==>>> epoch: 0, batch index: 900, train loss: 0.635672, acc: 0.642\n",
      "==>>> epoch: 0, batch index: 938, train loss: 0.608992, acc: 0.650\n",
      "==>>> epoch: 0, batch index: 100, test loss: 0.516358, acc: 0.809\n",
      "==>>> epoch: 0, batch index: 157, test loss: 0.596428, acc: 0.834\n",
      "==>>> epoch: 1, batch index: 100, train loss: 0.554468, acc: 0.838\n",
      "==>>> epoch: 1, batch index: 200, train loss: 0.555623, acc: 0.845\n",
      "==>>> epoch: 1, batch index: 300, train loss: 0.549781, acc: 0.847\n",
      "==>>> epoch: 1, batch index: 400, train loss: 0.491346, acc: 0.851\n",
      "==>>> epoch: 1, batch index: 500, train loss: 0.483176, acc: 0.854\n",
      "==>>> epoch: 1, batch index: 600, train loss: 0.435196, acc: 0.857\n",
      "==>>> epoch: 1, batch index: 700, train loss: 0.450888, acc: 0.860\n",
      "==>>> epoch: 1, batch index: 800, train loss: 0.376928, acc: 0.862\n",
      "==>>> epoch: 1, batch index: 900, train loss: 0.382237, acc: 0.865\n",
      "==>>> epoch: 1, batch index: 938, train loss: 0.410586, acc: 0.865\n",
      "==>>> epoch: 1, batch index: 100, test loss: 0.324341, acc: 0.872\n",
      "==>>> epoch: 1, batch index: 157, test loss: 0.376747, acc: 0.889\n",
      "==>>> epoch: 2, batch index: 100, train loss: 0.446743, acc: 0.883\n",
      "==>>> epoch: 2, batch index: 200, train loss: 0.397632, acc: 0.887\n",
      "==>>> epoch: 2, batch index: 300, train loss: 0.371484, acc: 0.888\n",
      "==>>> epoch: 2, batch index: 400, train loss: 0.387198, acc: 0.887\n",
      "==>>> epoch: 2, batch index: 500, train loss: 0.370463, acc: 0.890\n",
      "==>>> epoch: 2, batch index: 600, train loss: 0.361294, acc: 0.890\n",
      "==>>> epoch: 2, batch index: 700, train loss: 0.390053, acc: 0.890\n",
      "==>>> epoch: 2, batch index: 800, train loss: 0.361484, acc: 0.892\n",
      "==>>> epoch: 2, batch index: 900, train loss: 0.341508, acc: 0.892\n",
      "==>>> epoch: 2, batch index: 938, train loss: 0.377298, acc: 0.892\n",
      "==>>> epoch: 2, batch index: 100, test loss: 0.285279, acc: 0.888\n",
      "==>>> epoch: 2, batch index: 157, test loss: 0.320611, acc: 0.905\n",
      "==>>> epoch: 3, batch index: 100, train loss: 0.338080, acc: 0.900\n",
      "==>>> epoch: 3, batch index: 200, train loss: 0.355364, acc: 0.899\n",
      "==>>> epoch: 3, batch index: 300, train loss: 0.361920, acc: 0.900\n",
      "==>>> epoch: 3, batch index: 400, train loss: 0.318451, acc: 0.900\n",
      "==>>> epoch: 3, batch index: 500, train loss: 0.349484, acc: 0.900\n",
      "==>>> epoch: 3, batch index: 600, train loss: 0.347486, acc: 0.900\n",
      "==>>> epoch: 3, batch index: 700, train loss: 0.330052, acc: 0.900\n",
      "==>>> epoch: 3, batch index: 800, train loss: 0.324502, acc: 0.901\n",
      "==>>> epoch: 3, batch index: 900, train loss: 0.326908, acc: 0.902\n",
      "==>>> epoch: 3, batch index: 938, train loss: 0.310802, acc: 0.902\n",
      "==>>> epoch: 3, batch index: 100, test loss: 0.250522, acc: 0.895\n",
      "==>>> epoch: 3, batch index: 157, test loss: 0.310090, acc: 0.908\n",
      "==>>> epoch: 4, batch index: 100, train loss: 0.317869, acc: 0.902\n",
      "==>>> epoch: 4, batch index: 200, train loss: 0.365850, acc: 0.905\n",
      "==>>> epoch: 4, batch index: 300, train loss: 0.279488, acc: 0.907\n",
      "==>>> epoch: 4, batch index: 400, train loss: 0.318170, acc: 0.907\n",
      "==>>> epoch: 4, batch index: 500, train loss: 0.282722, acc: 0.908\n",
      "==>>> epoch: 4, batch index: 600, train loss: 0.318399, acc: 0.909\n",
      "==>>> epoch: 4, batch index: 700, train loss: 0.294721, acc: 0.909\n",
      "==>>> epoch: 4, batch index: 800, train loss: 0.313796, acc: 0.910\n",
      "==>>> epoch: 4, batch index: 900, train loss: 0.325590, acc: 0.910\n",
      "==>>> epoch: 4, batch index: 938, train loss: 0.289570, acc: 0.910\n",
      "==>>> epoch: 4, batch index: 100, test loss: 0.241171, acc: 0.905\n",
      "==>>> epoch: 4, batch index: 157, test loss: 0.284050, acc: 0.918\n",
      "==>>> epoch: 5, batch index: 100, train loss: 0.355292, acc: 0.911\n",
      "==>>> epoch: 5, batch index: 200, train loss: 0.317881, acc: 0.911\n",
      "==>>> epoch: 5, batch index: 300, train loss: 0.293853, acc: 0.913\n",
      "==>>> epoch: 5, batch index: 400, train loss: 0.287729, acc: 0.914\n",
      "==>>> epoch: 5, batch index: 500, train loss: 0.317017, acc: 0.915\n",
      "==>>> epoch: 5, batch index: 600, train loss: 0.318585, acc: 0.916\n",
      "==>>> epoch: 5, batch index: 700, train loss: 0.297679, acc: 0.916\n",
      "==>>> epoch: 5, batch index: 800, train loss: 0.266337, acc: 0.916\n",
      "==>>> epoch: 5, batch index: 900, train loss: 0.278800, acc: 0.916\n",
      "==>>> epoch: 5, batch index: 938, train loss: 0.280991, acc: 0.916\n",
      "==>>> epoch: 5, batch index: 100, test loss: 0.236083, acc: 0.907\n",
      "==>>> epoch: 5, batch index: 157, test loss: 0.293367, acc: 0.918\n",
      "==>>> epoch: 6, batch index: 100, train loss: 0.325104, acc: 0.917\n",
      "==>>> epoch: 6, batch index: 200, train loss: 0.276023, acc: 0.916\n",
      "==>>> epoch: 6, batch index: 300, train loss: 0.238854, acc: 0.918\n",
      "==>>> epoch: 6, batch index: 400, train loss: 0.286891, acc: 0.919\n",
      "==>>> epoch: 6, batch index: 500, train loss: 0.281700, acc: 0.919\n",
      "==>>> epoch: 6, batch index: 600, train loss: 0.267774, acc: 0.919\n",
      "==>>> epoch: 6, batch index: 700, train loss: 0.253331, acc: 0.919\n",
      "==>>> epoch: 6, batch index: 800, train loss: 0.269725, acc: 0.920\n",
      "==>>> epoch: 6, batch index: 900, train loss: 0.277936, acc: 0.920\n",
      "==>>> epoch: 6, batch index: 938, train loss: 0.248105, acc: 0.920\n",
      "==>>> epoch: 6, batch index: 100, test loss: 0.212794, acc: 0.912\n",
      "==>>> epoch: 6, batch index: 157, test loss: 0.252050, acc: 0.925\n",
      "==>>> epoch: 7, batch index: 100, train loss: 0.252082, acc: 0.921\n",
      "==>>> epoch: 7, batch index: 200, train loss: 0.284468, acc: 0.925\n",
      "==>>> epoch: 7, batch index: 300, train loss: 0.272280, acc: 0.924\n",
      "==>>> epoch: 7, batch index: 400, train loss: 0.293525, acc: 0.923\n",
      "==>>> epoch: 7, batch index: 500, train loss: 0.261491, acc: 0.923\n",
      "==>>> epoch: 7, batch index: 600, train loss: 0.268242, acc: 0.924\n",
      "==>>> epoch: 7, batch index: 700, train loss: 0.241767, acc: 0.924\n",
      "==>>> epoch: 7, batch index: 800, train loss: 0.251414, acc: 0.924\n",
      "==>>> epoch: 7, batch index: 900, train loss: 0.271257, acc: 0.925\n",
      "==>>> epoch: 7, batch index: 938, train loss: 0.228311, acc: 0.925\n",
      "==>>> epoch: 7, batch index: 100, test loss: 0.206642, acc: 0.913\n",
      "==>>> epoch: 7, batch index: 157, test loss: 0.247847, acc: 0.926\n",
      "==>>> epoch: 8, batch index: 100, train loss: 0.263363, acc: 0.927\n",
      "==>>> epoch: 8, batch index: 200, train loss: 0.235350, acc: 0.927\n",
      "==>>> epoch: 8, batch index: 300, train loss: 0.259834, acc: 0.926\n",
      "==>>> epoch: 8, batch index: 400, train loss: 0.278495, acc: 0.926\n",
      "==>>> epoch: 8, batch index: 500, train loss: 0.249370, acc: 0.926\n",
      "==>>> epoch: 8, batch index: 600, train loss: 0.204802, acc: 0.929\n",
      "==>>> epoch: 8, batch index: 700, train loss: 0.251999, acc: 0.929\n",
      "==>>> epoch: 8, batch index: 800, train loss: 0.228601, acc: 0.929\n",
      "==>>> epoch: 8, batch index: 900, train loss: 0.230733, acc: 0.929\n",
      "==>>> epoch: 8, batch index: 938, train loss: 0.230726, acc: 0.929\n",
      "==>>> epoch: 8, batch index: 100, test loss: 0.195888, acc: 0.922\n",
      "==>>> epoch: 8, batch index: 157, test loss: 0.228192, acc: 0.932\n",
      "==>>> epoch: 9, batch index: 100, train loss: 0.228941, acc: 0.930\n",
      "==>>> epoch: 9, batch index: 200, train loss: 0.247200, acc: 0.929\n",
      "==>>> epoch: 9, batch index: 300, train loss: 0.222944, acc: 0.930\n",
      "==>>> epoch: 9, batch index: 400, train loss: 0.234063, acc: 0.931\n",
      "==>>> epoch: 9, batch index: 500, train loss: 0.228662, acc: 0.932\n",
      "==>>> epoch: 9, batch index: 600, train loss: 0.246345, acc: 0.932\n",
      "==>>> epoch: 9, batch index: 700, train loss: 0.196517, acc: 0.932\n",
      "==>>> epoch: 9, batch index: 800, train loss: 0.260039, acc: 0.933\n",
      "==>>> epoch: 9, batch index: 900, train loss: 0.227795, acc: 0.933\n",
      "==>>> epoch: 9, batch index: 938, train loss: 0.227588, acc: 0.933\n",
      "==>>> epoch: 9, batch index: 100, test loss: 0.181137, acc: 0.918\n",
      "==>>> epoch: 9, batch index: 157, test loss: 0.228528, acc: 0.930\n",
      "==>>> epoch: 10, batch index: 100, train loss: 0.266828, acc: 0.934\n",
      "==>>> epoch: 10, batch index: 200, train loss: 0.228877, acc: 0.935\n",
      "==>>> epoch: 10, batch index: 300, train loss: 0.221010, acc: 0.933\n",
      "==>>> epoch: 10, batch index: 400, train loss: 0.221365, acc: 0.934\n",
      "==>>> epoch: 10, batch index: 500, train loss: 0.231524, acc: 0.935\n",
      "==>>> epoch: 10, batch index: 600, train loss: 0.226863, acc: 0.936\n",
      "==>>> epoch: 10, batch index: 700, train loss: 0.248404, acc: 0.936\n",
      "==>>> epoch: 10, batch index: 800, train loss: 0.189525, acc: 0.936\n",
      "==>>> epoch: 10, batch index: 900, train loss: 0.224583, acc: 0.937\n",
      "==>>> epoch: 10, batch index: 938, train loss: 0.198590, acc: 0.937\n",
      "==>>> epoch: 10, batch index: 100, test loss: 0.186115, acc: 0.925\n",
      "==>>> epoch: 10, batch index: 157, test loss: 0.209912, acc: 0.937\n",
      "==>>> epoch: 11, batch index: 100, train loss: 0.198247, acc: 0.943\n",
      "==>>> epoch: 11, batch index: 200, train loss: 0.211062, acc: 0.941\n",
      "==>>> epoch: 11, batch index: 300, train loss: 0.218040, acc: 0.941\n",
      "==>>> epoch: 11, batch index: 400, train loss: 0.211386, acc: 0.940\n",
      "==>>> epoch: 11, batch index: 500, train loss: 0.211741, acc: 0.940\n",
      "==>>> epoch: 11, batch index: 600, train loss: 0.170694, acc: 0.941\n",
      "==>>> epoch: 11, batch index: 700, train loss: 0.175888, acc: 0.941\n",
      "==>>> epoch: 11, batch index: 800, train loss: 0.229706, acc: 0.940\n",
      "==>>> epoch: 11, batch index: 900, train loss: 0.202663, acc: 0.940\n",
      "==>>> epoch: 11, batch index: 938, train loss: 0.214126, acc: 0.941\n",
      "==>>> epoch: 11, batch index: 100, test loss: 0.159542, acc: 0.930\n",
      "==>>> epoch: 11, batch index: 157, test loss: 0.196282, acc: 0.941\n",
      "==>>> epoch: 12, batch index: 100, train loss: 0.204260, acc: 0.944\n",
      "==>>> epoch: 12, batch index: 200, train loss: 0.203200, acc: 0.944\n",
      "==>>> epoch: 12, batch index: 300, train loss: 0.186055, acc: 0.944\n",
      "==>>> epoch: 12, batch index: 400, train loss: 0.197404, acc: 0.943\n",
      "==>>> epoch: 12, batch index: 500, train loss: 0.201021, acc: 0.944\n",
      "==>>> epoch: 12, batch index: 600, train loss: 0.221323, acc: 0.944\n",
      "==>>> epoch: 12, batch index: 700, train loss: 0.189310, acc: 0.944\n",
      "==>>> epoch: 12, batch index: 800, train loss: 0.177143, acc: 0.944\n",
      "==>>> epoch: 12, batch index: 900, train loss: 0.196625, acc: 0.944\n",
      "==>>> epoch: 12, batch index: 938, train loss: 0.192110, acc: 0.944\n",
      "==>>> epoch: 12, batch index: 100, test loss: 0.156066, acc: 0.934\n",
      "==>>> epoch: 12, batch index: 157, test loss: 0.187651, acc: 0.944\n",
      "==>>> epoch: 13, batch index: 100, train loss: 0.192599, acc: 0.948\n",
      "==>>> epoch: 13, batch index: 200, train loss: 0.197103, acc: 0.946\n",
      "==>>> epoch: 13, batch index: 300, train loss: 0.163064, acc: 0.948\n",
      "==>>> epoch: 13, batch index: 400, train loss: 0.197657, acc: 0.948\n",
      "==>>> epoch: 13, batch index: 500, train loss: 0.217861, acc: 0.948\n",
      "==>>> epoch: 13, batch index: 600, train loss: 0.179294, acc: 0.947\n",
      "==>>> epoch: 13, batch index: 700, train loss: 0.182406, acc: 0.947\n",
      "==>>> epoch: 13, batch index: 800, train loss: 0.166351, acc: 0.948\n",
      "==>>> epoch: 13, batch index: 900, train loss: 0.200547, acc: 0.948\n",
      "==>>> epoch: 13, batch index: 938, train loss: 0.202115, acc: 0.948\n",
      "==>>> epoch: 13, batch index: 100, test loss: 0.151352, acc: 0.937\n",
      "==>>> epoch: 13, batch index: 157, test loss: 0.179314, acc: 0.947\n",
      "==>>> epoch: 14, batch index: 100, train loss: 0.186928, acc: 0.950\n",
      "==>>> epoch: 14, batch index: 200, train loss: 0.176071, acc: 0.950\n",
      "==>>> epoch: 14, batch index: 300, train loss: 0.202909, acc: 0.949\n",
      "==>>> epoch: 14, batch index: 400, train loss: 0.154116, acc: 0.949\n",
      "==>>> epoch: 14, batch index: 500, train loss: 0.148853, acc: 0.949\n",
      "==>>> epoch: 14, batch index: 600, train loss: 0.175742, acc: 0.950\n",
      "==>>> epoch: 14, batch index: 700, train loss: 0.200608, acc: 0.950\n",
      "==>>> epoch: 14, batch index: 800, train loss: 0.141013, acc: 0.950\n",
      "==>>> epoch: 14, batch index: 900, train loss: 0.202948, acc: 0.949\n",
      "==>>> epoch: 14, batch index: 938, train loss: 0.136900, acc: 0.950\n",
      "==>>> epoch: 14, batch index: 100, test loss: 0.142640, acc: 0.938\n",
      "==>>> epoch: 14, batch index: 157, test loss: 0.174873, acc: 0.949\n",
      "==>>> epoch: 15, batch index: 100, train loss: 0.143454, acc: 0.952\n",
      "==>>> epoch: 15, batch index: 200, train loss: 0.187689, acc: 0.950\n",
      "==>>> epoch: 15, batch index: 300, train loss: 0.161384, acc: 0.952\n",
      "==>>> epoch: 15, batch index: 400, train loss: 0.147206, acc: 0.951\n",
      "==>>> epoch: 15, batch index: 500, train loss: 0.184251, acc: 0.951\n",
      "==>>> epoch: 15, batch index: 600, train loss: 0.167646, acc: 0.952\n",
      "==>>> epoch: 15, batch index: 700, train loss: 0.166040, acc: 0.952\n",
      "==>>> epoch: 15, batch index: 800, train loss: 0.196295, acc: 0.952\n",
      "==>>> epoch: 15, batch index: 900, train loss: 0.152642, acc: 0.953\n",
      "==>>> epoch: 15, batch index: 938, train loss: 0.140763, acc: 0.953\n",
      "==>>> epoch: 15, batch index: 100, test loss: 0.138734, acc: 0.943\n",
      "==>>> epoch: 15, batch index: 157, test loss: 0.168167, acc: 0.951\n",
      "==>>> epoch: 16, batch index: 100, train loss: 0.186289, acc: 0.956\n",
      "==>>> epoch: 16, batch index: 200, train loss: 0.150390, acc: 0.954\n",
      "==>>> epoch: 16, batch index: 300, train loss: 0.167257, acc: 0.954\n",
      "==>>> epoch: 16, batch index: 400, train loss: 0.162184, acc: 0.954\n",
      "==>>> epoch: 16, batch index: 500, train loss: 0.168768, acc: 0.954\n",
      "==>>> epoch: 16, batch index: 600, train loss: 0.185574, acc: 0.954\n",
      "==>>> epoch: 16, batch index: 700, train loss: 0.126577, acc: 0.954\n",
      "==>>> epoch: 16, batch index: 800, train loss: 0.154259, acc: 0.955\n",
      "==>>> epoch: 16, batch index: 900, train loss: 0.138038, acc: 0.955\n",
      "==>>> epoch: 16, batch index: 938, train loss: 0.156712, acc: 0.955\n",
      "==>>> epoch: 16, batch index: 100, test loss: 0.134500, acc: 0.940\n",
      "==>>> epoch: 16, batch index: 157, test loss: 0.159535, acc: 0.951\n",
      "==>>> epoch: 17, batch index: 100, train loss: 0.149954, acc: 0.954\n",
      "==>>> epoch: 17, batch index: 200, train loss: 0.142074, acc: 0.954\n",
      "==>>> epoch: 17, batch index: 300, train loss: 0.120980, acc: 0.954\n",
      "==>>> epoch: 17, batch index: 400, train loss: 0.119498, acc: 0.956\n",
      "==>>> epoch: 17, batch index: 500, train loss: 0.172940, acc: 0.957\n",
      "==>>> epoch: 17, batch index: 600, train loss: 0.160893, acc: 0.958\n",
      "==>>> epoch: 17, batch index: 700, train loss: 0.142585, acc: 0.958\n",
      "==>>> epoch: 17, batch index: 800, train loss: 0.151292, acc: 0.958\n",
      "==>>> epoch: 17, batch index: 900, train loss: 0.138592, acc: 0.957\n",
      "==>>> epoch: 17, batch index: 938, train loss: 0.151394, acc: 0.957\n",
      "==>>> epoch: 17, batch index: 100, test loss: 0.127793, acc: 0.945\n",
      "==>>> epoch: 17, batch index: 157, test loss: 0.150984, acc: 0.955\n",
      "==>>> epoch: 18, batch index: 100, train loss: 0.138465, acc: 0.960\n",
      "==>>> epoch: 18, batch index: 200, train loss: 0.167027, acc: 0.958\n",
      "==>>> epoch: 18, batch index: 300, train loss: 0.142306, acc: 0.960\n",
      "==>>> epoch: 18, batch index: 400, train loss: 0.155839, acc: 0.959\n",
      "==>>> epoch: 18, batch index: 500, train loss: 0.145769, acc: 0.960\n",
      "==>>> epoch: 18, batch index: 600, train loss: 0.173794, acc: 0.959\n",
      "==>>> epoch: 18, batch index: 700, train loss: 0.131204, acc: 0.960\n",
      "==>>> epoch: 18, batch index: 800, train loss: 0.134249, acc: 0.960\n",
      "==>>> epoch: 18, batch index: 900, train loss: 0.127050, acc: 0.960\n",
      "==>>> epoch: 18, batch index: 938, train loss: 0.156631, acc: 0.960\n",
      "==>>> epoch: 18, batch index: 100, test loss: 0.119314, acc: 0.950\n",
      "==>>> epoch: 18, batch index: 157, test loss: 0.144027, acc: 0.958\n",
      "==>>> epoch: 19, batch index: 100, train loss: 0.163954, acc: 0.958\n",
      "==>>> epoch: 19, batch index: 200, train loss: 0.139031, acc: 0.959\n",
      "==>>> epoch: 19, batch index: 300, train loss: 0.116146, acc: 0.961\n",
      "==>>> epoch: 19, batch index: 400, train loss: 0.122965, acc: 0.961\n",
      "==>>> epoch: 19, batch index: 500, train loss: 0.143298, acc: 0.960\n",
      "==>>> epoch: 19, batch index: 600, train loss: 0.127562, acc: 0.961\n",
      "==>>> epoch: 19, batch index: 700, train loss: 0.139459, acc: 0.961\n",
      "==>>> epoch: 19, batch index: 800, train loss: 0.136270, acc: 0.961\n",
      "==>>> epoch: 19, batch index: 900, train loss: 0.121562, acc: 0.962\n",
      "==>>> epoch: 19, batch index: 938, train loss: 0.142489, acc: 0.961\n",
      "==>>> epoch: 19, batch index: 100, test loss: 0.120207, acc: 0.950\n",
      "==>>> epoch: 19, batch index: 157, test loss: 0.139644, acc: 0.959\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(20):\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.shape[0]\n",
    "        correct_cnt+= (pred_label == target).sum().item()\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss, correct_cnt*1.0/total_cnt))\n",
    "    # testing\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "    for batch_idx, (x, target) in enumerate(test_loader):\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.shape[0]\n",
    "#         print(target.data)\n",
    "        correct_cnt += (pred_label == target).sum().item()\n",
    "        # smooth average\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "        \n",
    "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss, correct_cnt * 1.0 / total_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
